{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read train and test data\n",
    "train_data = pd.read_excel('训练.xlsx')\n",
    "test_data_1 = pd.read_excel('测试A.xlsx')\n",
    "test_data_2 = pd.read_excel('测试B.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_nonnull = train_data.isnull().values\n",
    "test_nonnull_1 = test_data_1.isnull().values\n",
    "test_nonnull_2 = test_data_2.isnull().values\n",
    "train_X = train_data.values\n",
    "test_X_1 = test_data_1.values\n",
    "test_X_2 = test_data_2.values\n",
    "test_id_1 = test_X_1[:, 0]\n",
    "test_id_2 = test_X_2[:, 0]\n",
    "train_y = train_X[:, -1]\n",
    "train_X = train_X[:, 1:-1]\n",
    "test_X_1 = test_X_1[:, 1:]\n",
    "test_X_2 = test_X_2[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 8027), (100, 8027), (121, 8027))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X_1.shape, test_X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 8027) (721, 8027)\n"
     ]
    }
   ],
   "source": [
    "#Merge two data\n",
    "merged_X = np.concatenate((train_X, test_X_1, test_X_2), axis=0)\n",
    "merged_nn = np.concatenate((train_nonnull[:, 1:-1], test_nonnull_1[:, 1:], test_nonnull_2[:, 1:]), axis=0)\n",
    "print(merged_X.shape, merged_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reverse nn\n",
    "def BoolReverse(columns):\n",
    "    tmpm = np.zeros(columns.shape)\n",
    "    tmpm[columns] = 1\n",
    "    return tmpm == 0\n",
    "\n",
    "merged_nn = BoolReverse(merged_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "# now find the values that has 0 value and delete such column\n",
    "first_delete_idxs = []\n",
    "for i in range(merged_X.shape[1]):\n",
    "    data = merged_X[:, i]\n",
    "    real = merged_nn[:, i]\n",
    "    rd = data[real]\n",
    "    if len(rd) == 0:\n",
    "        first_delete_idxs.append(i)\n",
    "    elif len(set(rd)) == 0:\n",
    "        first_delete_idxs.append(i)\n",
    "print(len(first_delete_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 8027)\n",
      "(721, 7965)\n"
     ]
    }
   ],
   "source": [
    "print(merged_X.shape)\n",
    "merged_X = np.delete(merged_X, first_delete_idxs, axis=1)\n",
    "print(merged_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 8027)\n",
      "(721, 7965)\n"
     ]
    }
   ],
   "source": [
    "print(merged_nn.shape)\n",
    "merged_nn = np.delete(merged_nn, first_delete_idxs, axis=1)\n",
    "print(merged_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007\n"
     ]
    }
   ],
   "source": [
    "#now find the columns that has only one value\n",
    "only_one_values = 0\n",
    "for i in range(merged_X.shape[1]):\n",
    "    data = merged_X[:, i]\n",
    "    real = merged_nn[:, i]\n",
    "    rd = data[real]\n",
    "    if len(set(rd)) == 1:\n",
    "        merged_X[:, i] = 1.0\n",
    "        merged_nn[:, i] = True\n",
    "        only_one_values += 1\n",
    "print(only_one_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict2list(dic):\n",
    "    keys = dic.keys()\n",
    "    vals = dic.values()\n",
    "    lst = [(key, val) for key, val in zip(keys, vals)]\n",
    "    return lst\n",
    "\n",
    "# input one array and return the most frequent one\n",
    "def MostFrequentOne(column):\n",
    "    itemfreq = {}\n",
    "    for i in column:\n",
    "        if i not in itemfreq:\n",
    "            itemfreq[i] = 1\n",
    "        else:\n",
    "            itemfreq[i] += 1\n",
    "    tmp_dict = sorted(dict2list(itemfreq), key=lambda d:d[1], reverse=True)\n",
    "    return tmp_dict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one_hot to map str to int\n",
    "def OneHot_Str(column, retdict = 0):\n",
    "    syms = set(column)\n",
    "    sim = {}\n",
    "    count = 0\n",
    "    for s in syms:\n",
    "        sim[s] = count\n",
    "        count += 1\n",
    "    cl = len(column)\n",
    "    retoh = np.zeros((cl, count))\n",
    "    for i in range(cl):\n",
    "        s = column[i]\n",
    "        mi = sim[s]\n",
    "        retoh[i, mi] = 1.0\n",
    "    if retdict == 0:\n",
    "        return retoh\n",
    "    else:\n",
    "        return retoh, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(721, 7999)\n",
      "(721, 7989)\n"
     ]
    }
   ],
   "source": [
    "#now fix the remain data\n",
    "#if the values in data has one value that is string, then one_hot it, else normalize it\n",
    "stridxs = []\n",
    "for i in range(merged_X.shape[1]):\n",
    "    data = merged_X[:, i]\n",
    "    real = merged_nn[:, i]\n",
    "    rd = data[real]\n",
    "    #if len(data) == len(rd) and np.mean(data) == 1.0:\n",
    "    #    continue\n",
    "    isstr = False\n",
    "    for j in range(len(rd)):\n",
    "        x = rd[j]\n",
    "        sx = str(type(x))\n",
    "        if 'str' in sx:\n",
    "            isstr = True\n",
    "            break\n",
    "    if isstr:\n",
    "        stridxs.append(i)\n",
    "        # fix null and one_hot it\n",
    "        if len(rd) < len(data):\n",
    "            mfc = MostFrequentOne(rd)\n",
    "            rn = BoolReverse(real)\n",
    "            data[rn] = mfc\n",
    "        onehots = OneHot_Str(data)\n",
    "        merged_X = np.concatenate((merged_X, onehots), axis=1)\n",
    "    else:\n",
    "        if len(rd) < len(data):\n",
    "            rn = BoolReverse(real)\n",
    "            data[rn] = np.mean(rd)\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            data = (data-mean)/std\n",
    "            merged_X[:, i] = data\n",
    "#delete the string columns\n",
    "print(len(stridxs))\n",
    "print(merged_X.shape)\n",
    "merged_X = np.delete(merged_X, stridxs, axis=1)\n",
    "print(merged_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now use pca to reduce its demension\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.999999975997\n",
      "1 0.999999988161\n",
      "2 0.999999996208\n",
      "3 0.999999999192\n",
      "4 0.999999999921\n",
      "5 0.999999999973\n",
      "6 0.999999999996\n",
      "7 0.999999999998\n",
      "8 0.999999999999\n",
      "9 0.999999999999\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 1.0\n",
      "new shape (721, 15)\n"
     ]
    }
   ],
   "source": [
    "cn = 15 \n",
    "pca = PCA(n_components=cn) \n",
    "pca.fit(merged_X) \n",
    "psum = 0 \n",
    "pe = pca.explained_variance_ratio_\n",
    "for i in range(cn): \n",
    "    e = pe[i] \n",
    "    psum += e \n",
    "    print(i, psum) \n",
    "X_new = pca.transform(merged_X) \n",
    "print(\"new shape\", X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize y\n",
    "y_max = np.max(train_y)\n",
    "y_min = np.min(train_y)\n",
    "real_y = (train_y-y_min)/(y_max-y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize new_X\n",
    "for i in range(cn):\n",
    "    col = X_new[:, i]\n",
    "    m = np.mean(col)\n",
    "    s = np.std(col)\n",
    "    col = (col-m)/s\n",
    "    X_new[:, i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 15) (100, 15) (121, 15)\n"
     ]
    }
   ],
   "source": [
    "# split train, test1, test2 data\n",
    "trainX = X_new[:500]\n",
    "test1X = X_new[500:600]\n",
    "test2X = X_new[600:]\n",
    "print(trainX.shape, test1X.shape, test2X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modeling and predicting\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(input_data):\n",
    "    l1 = tf.layers.dense(input_data, 1)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = tf.placeholder(shape=[None, cn], dtype=tf.float32)\n",
    "input_y = tf.placeholder(shape=[None], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = model(input_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_f(y, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y-y_pred))\n",
    "\n",
    "def MSE(y, y_pred):\n",
    "    #yp = y_pred*(y_max-y_min)+y_min\n",
    "    return tf.reduce_mean(tf.square(y-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = loss_f(input_y, pred_y) \n",
    "mse = MSE(input_y, pred_y) \n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0426059\n",
      "0.0425968\n",
      "1 0.0425968\n",
      "2 0.0425878\n",
      "3 0.042579\n",
      "4 0.0425701\n",
      "5 0.0425611\n",
      "6 0.0425521\n",
      "7 0.0425436\n",
      "8 0.0425351\n",
      "9 0.0425264\n",
      "10 0.0425175\n",
      "0.0425089\n",
      "11 0.0425089\n",
      "12 0.0425001\n",
      "13 0.0424916\n",
      "14 0.042483\n",
      "15 0.0424746\n",
      "16 0.0424661\n",
      "17 0.0424577\n",
      "18 0.0424491\n",
      "19 0.0424404\n",
      "20 0.0424323\n",
      "0.042424\n",
      "21 0.042424\n",
      "22 0.0424156\n",
      "23 0.0424074\n",
      "24 0.0423992\n",
      "25 0.042391\n",
      "26 0.0423828\n",
      "27 0.0423747\n",
      "28 0.0423666\n",
      "29 0.0423583\n",
      "30 0.0423505\n",
      "0.0423424\n",
      "31 0.0423424\n",
      "32 0.0423344\n",
      "33 0.0423264\n",
      "34 0.0423184\n",
      "35 0.0423105\n",
      "36 0.0423024\n",
      "37 0.0422945\n",
      "38 0.0422867\n",
      "39 0.0422789\n",
      "40 0.0422709\n",
      "0.0422632\n",
      "41 0.0422632\n",
      "42 0.0422553\n",
      "43 0.0422475\n",
      "44 0.04224\n",
      "45 0.0422321\n",
      "46 0.0422246\n",
      "47 0.0422169\n",
      "48 0.0422094\n",
      "49 0.0422019\n",
      "50 0.0421944\n",
      "0.0421868\n",
      "51 0.0421868\n",
      "52 0.0421792\n",
      "53 0.0421716\n",
      "54 0.0421642\n",
      "55 0.0421569\n",
      "56 0.0421493\n",
      "57 0.0421422\n",
      "58 0.0421348\n",
      "59 0.0421276\n",
      "60 0.0421202\n",
      "0.0421127\n",
      "61 0.0421127\n",
      "62 0.0421058\n",
      "63 0.0420984\n",
      "64 0.0420913\n",
      "65 0.0420843\n",
      "66 0.042077\n",
      "67 0.0420698\n",
      "68 0.0420627\n",
      "69 0.0420556\n",
      "70 0.0420488\n",
      "0.0420415\n",
      "71 0.0420415\n",
      "72 0.0420347\n",
      "73 0.0420277\n",
      "74 0.0420207\n",
      "75 0.0420138\n",
      "76 0.042007\n",
      "77 0.0420002\n",
      "78 0.0419933\n",
      "79 0.0419868\n",
      "80 0.04198\n",
      "0.0419731\n",
      "81 0.0419731\n",
      "82 0.0419663\n",
      "83 0.0419597\n",
      "84 0.0419529\n",
      "85 0.0419462\n",
      "86 0.0419397\n",
      "87 0.0419328\n",
      "88 0.0419262\n",
      "89 0.0419198\n",
      "90 0.0419129\n",
      "0.0419064\n",
      "91 0.0419064\n",
      "92 0.0419\n",
      "93 0.0418935\n",
      "94 0.0418869\n",
      "95 0.0418804\n",
      "96 0.0418741\n",
      "97 0.0418679\n",
      "98 0.0418613\n",
      "99 0.0418549\n",
      "100 0.0418488\n",
      "0.0418424\n",
      "101 0.0418424\n",
      "102 0.0418361\n",
      "103 0.0418299\n",
      "104 0.0418237\n",
      "105 0.0418174\n",
      "106 0.0418113\n",
      "107 0.041805\n",
      "108 0.0417988\n",
      "109 0.0417928\n",
      "110 0.0417868\n",
      "0.0417808\n",
      "111 0.0417808\n",
      "112 0.0417747\n",
      "113 0.0417687\n",
      "114 0.0417625\n",
      "115 0.0417564\n",
      "116 0.0417506\n",
      "117 0.0417444\n",
      "118 0.0417388\n",
      "119 0.0417328\n",
      "120 0.0417271\n",
      "0.041721\n",
      "121 0.041721\n",
      "122 0.0417152\n",
      "123 0.0417095\n",
      "124 0.0417036\n",
      "125 0.0416979\n",
      "126 0.0416921\n",
      "127 0.0416865\n",
      "128 0.0416808\n",
      "129 0.0416751\n",
      "130 0.0416695\n",
      "0.0416635\n",
      "131 0.0416635\n",
      "132 0.0416579\n",
      "133 0.0416524\n",
      "134 0.0416466\n",
      "135 0.041641\n",
      "136 0.0416356\n",
      "137 0.0416298\n",
      "138 0.0416246\n",
      "139 0.0416189\n",
      "140 0.0416135\n",
      "0.0416081\n",
      "141 0.0416081\n",
      "142 0.0416026\n",
      "143 0.0415973\n",
      "144 0.0415919\n",
      "145 0.0415868\n",
      "146 0.0415813\n",
      "147 0.0415761\n",
      "148 0.0415707\n",
      "149 0.0415653\n",
      "150 0.0415598\n",
      "0.0415547\n",
      "151 0.0415547\n",
      "152 0.0415496\n",
      "153 0.0415443\n",
      "154 0.041539\n",
      "155 0.0415339\n",
      "156 0.0415286\n",
      "157 0.0415237\n",
      "158 0.0415188\n",
      "159 0.0415134\n",
      "160 0.0415084\n",
      "0.0415034\n",
      "161 0.0415034\n",
      "162 0.0414982\n",
      "163 0.0414931\n",
      "164 0.041488\n",
      "165 0.0414834\n",
      "166 0.0414782\n",
      "167 0.0414733\n",
      "168 0.0414682\n",
      "169 0.0414634\n",
      "170 0.0414587\n",
      "0.0414538\n",
      "171 0.0414538\n",
      "172 0.0414488\n",
      "173 0.0414439\n",
      "174 0.0414392\n",
      "175 0.0414345\n",
      "176 0.0414298\n",
      "177 0.041425\n",
      "178 0.0414199\n",
      "179 0.0414155\n",
      "180 0.0414104\n",
      "0.041406\n",
      "181 0.041406\n",
      "182 0.0414011\n",
      "183 0.0413964\n",
      "184 0.0413919\n",
      "185 0.0413873\n",
      "186 0.0413825\n",
      "187 0.0413781\n",
      "188 0.0413737\n",
      "189 0.0413689\n",
      "190 0.0413644\n",
      "0.0413599\n",
      "191 0.0413599\n",
      "192 0.0413553\n",
      "193 0.0413508\n",
      "194 0.0413462\n",
      "195 0.0413419\n",
      "196 0.0413375\n",
      "197 0.0413333\n",
      "198 0.0413288\n",
      "199 0.0413242\n",
      "200 0.0413201\n",
      "0.0413155\n",
      "201 0.0413155\n",
      "202 0.041311\n",
      "203 0.0413067\n",
      "204 0.0413026\n",
      "205 0.0412983\n",
      "206 0.0412939\n",
      "207 0.0412899\n",
      "208 0.0412857\n",
      "209 0.0412813\n",
      "210 0.0412772\n",
      "0.0412731\n",
      "211 0.0412731\n",
      "212 0.0412689\n",
      "213 0.0412648\n",
      "214 0.0412606\n",
      "215 0.0412564\n",
      "216 0.0412523\n",
      "217 0.0412481\n",
      "218 0.041244\n",
      "219 0.0412399\n",
      "220 0.0412358\n",
      "0.0412317\n",
      "221 0.0412317\n",
      "222 0.0412277\n",
      "223 0.0412235\n",
      "224 0.0412198\n",
      "225 0.041216\n",
      "226 0.041212\n",
      "227 0.0412082\n",
      "228 0.0412042\n",
      "229 0.0412001\n",
      "230 0.0411963\n",
      "0.0411925\n",
      "231 0.0411925\n",
      "232 0.0411886\n",
      "233 0.0411847\n",
      "234 0.0411808\n",
      "235 0.0411769\n",
      "236 0.0411732\n",
      "237 0.0411695\n",
      "238 0.0411656\n",
      "239 0.0411618\n",
      "240 0.0411581\n",
      "0.0411543\n",
      "241 0.0411543\n",
      "242 0.0411505\n",
      "243 0.0411469\n",
      "244 0.0411433\n",
      "245 0.0411397\n",
      "246 0.0411359\n",
      "247 0.0411323\n",
      "248 0.0411286\n",
      "249 0.0411248\n",
      "250 0.0411212\n",
      "0.0411177\n",
      "251 0.0411177\n",
      "252 0.0411142\n",
      "253 0.0411104\n",
      "254 0.0411071\n",
      "255 0.0411037\n",
      "256 0.0411\n",
      "257 0.0410965\n",
      "258 0.0410931\n",
      "259 0.0410895\n",
      "260 0.0410861\n",
      "0.0410825\n",
      "261 0.0410825\n",
      "262 0.0410794\n",
      "263 0.0410758\n",
      "264 0.0410726\n",
      "265 0.0410693\n",
      "266 0.0410655\n",
      "267 0.041062\n",
      "268 0.0410588\n",
      "269 0.0410555\n",
      "270 0.0410522\n",
      "0.0410489\n",
      "271 0.0410489\n",
      "272 0.0410456\n",
      "273 0.0410424\n",
      "274 0.041039\n",
      "275 0.0410355\n",
      "276 0.0410322\n",
      "277 0.041029\n",
      "278 0.0410257\n",
      "279 0.0410227\n",
      "280 0.0410196\n",
      "0.0410166\n",
      "281 0.0410166\n",
      "282 0.0410134\n",
      "283 0.04101\n",
      "284 0.0410069\n",
      "285 0.0410038\n",
      "286 0.0410005\n",
      "287 0.0409974\n",
      "288 0.0409944\n",
      "289 0.0409914\n",
      "290 0.0409884\n",
      "0.0409853\n",
      "291 0.0409853\n",
      "292 0.0409824\n",
      "293 0.0409792\n",
      "294 0.040976\n",
      "295 0.0409731\n",
      "296 0.0409699\n",
      "297 0.0409669\n",
      "298 0.040964\n",
      "299 0.040961\n",
      "300 0.0409581\n",
      "0.0409552\n",
      "301 0.0409552\n",
      "302 0.0409522\n",
      "303 0.0409492\n",
      "304 0.0409464\n",
      "305 0.0409434\n",
      "306 0.0409404\n",
      "307 0.0409378\n",
      "308 0.0409349\n",
      "309 0.0409321\n",
      "310 0.0409292\n",
      "0.0409265\n",
      "311 0.0409265\n",
      "312 0.0409236\n",
      "313 0.0409209\n",
      "314 0.0409181\n",
      "315 0.0409152\n",
      "316 0.0409124\n",
      "317 0.0409097\n",
      "318 0.0409068\n",
      "319 0.0409041\n",
      "320 0.0409015\n",
      "0.0408988\n",
      "321 0.0408988\n",
      "322 0.0408961\n",
      "323 0.0408936\n",
      "324 0.0408907\n",
      "325 0.0408879\n",
      "326 0.0408854\n",
      "327 0.0408828\n",
      "328 0.0408801\n",
      "329 0.0408775\n",
      "330 0.0408748\n",
      "0.0408721\n",
      "331 0.0408721\n",
      "332 0.0408694\n",
      "333 0.0408671\n",
      "334 0.0408643\n",
      "335 0.0408618\n",
      "336 0.0408592\n",
      "337 0.0408568\n",
      "338 0.0408544\n",
      "339 0.0408515\n",
      "340 0.040849\n",
      "0.0408466\n",
      "341 0.0408466\n",
      "342 0.0408442\n",
      "343 0.0408416\n",
      "344 0.0408391\n",
      "345 0.0408366\n",
      "346 0.0408344\n",
      "347 0.0408321\n",
      "348 0.0408297\n",
      "349 0.0408271\n",
      "350 0.0408247\n",
      "0.0408223\n",
      "351 0.0408223\n",
      "352 0.0408197\n",
      "353 0.0408175\n",
      "354 0.0408152\n",
      "355 0.0408127\n",
      "356 0.0408104\n",
      "357 0.0408081\n",
      "358 0.0408057\n",
      "359 0.0408033\n",
      "360 0.040801\n",
      "0.0407989\n",
      "361 0.0407989\n",
      "362 0.0407966\n",
      "363 0.0407943\n",
      "364 0.0407921\n",
      "365 0.0407898\n",
      "366 0.0407877\n",
      "367 0.0407853\n",
      "368 0.0407831\n",
      "369 0.0407808\n",
      "370 0.0407784\n",
      "0.0407762\n",
      "371 0.0407762\n",
      "372 0.040774\n",
      "373 0.0407717\n",
      "374 0.0407697\n",
      "375 0.0407673\n",
      "376 0.0407652\n",
      "377 0.0407632\n",
      "378 0.0407609\n",
      "379 0.0407589\n",
      "380 0.0407569\n",
      "0.040755\n",
      "381 0.040755\n",
      "382 0.0407527\n",
      "383 0.0407504\n",
      "384 0.0407484\n",
      "385 0.0407463\n",
      "386 0.040744\n",
      "387 0.040742\n",
      "388 0.0407401\n",
      "389 0.0407381\n",
      "390 0.0407362\n",
      "0.0407342\n",
      "391 0.0407342\n",
      "392 0.0407321\n",
      "393 0.0407302\n",
      "394 0.0407281\n",
      "395 0.0407261\n",
      "396 0.040724\n",
      "397 0.040722\n",
      "398 0.0407201\n",
      "399 0.040718\n",
      "400 0.0407162\n",
      "0.0407142\n",
      "401 0.0407142\n",
      "402 0.0407123\n",
      "403 0.0407102\n",
      "404 0.0407083\n",
      "405 0.0407065\n",
      "406 0.0407046\n",
      "407 0.0407027\n",
      "408 0.0407009\n",
      "409 0.0406988\n",
      "410 0.0406971\n",
      "0.0406953\n",
      "411 0.0406953\n",
      "412 0.0406935\n",
      "413 0.0406915\n",
      "414 0.0406895\n",
      "415 0.0406879\n",
      "416 0.0406859\n",
      "417 0.0406841\n",
      "418 0.0406823\n",
      "419 0.0406804\n",
      "420 0.0406786\n",
      "0.0406767\n",
      "421 0.0406767\n",
      "422 0.040675\n",
      "423 0.0406736\n",
      "424 0.0406719\n",
      "425 0.04067\n",
      "426 0.0406681\n",
      "427 0.0406665\n",
      "428 0.0406646\n",
      "429 0.040663\n",
      "430 0.0406614\n",
      "0.0406596\n",
      "431 0.0406596\n",
      "432 0.0406578\n",
      "433 0.040656\n",
      "434 0.0406542\n",
      "435 0.040653\n",
      "436 0.0406511\n",
      "437 0.0406494\n",
      "438 0.0406477\n",
      "439 0.0406461\n",
      "440 0.0406444\n",
      "0.0406429\n",
      "441 0.0406429\n",
      "442 0.0406412\n",
      "443 0.0406397\n",
      "444 0.0406379\n",
      "445 0.0406364\n",
      "446 0.0406348\n",
      "447 0.0406331\n",
      "448 0.0406314\n",
      "449 0.0406298\n",
      "450 0.0406282\n",
      "0.0406267\n",
      "451 0.0406267\n",
      "452 0.0406251\n",
      "453 0.0406237\n",
      "454 0.0406221\n",
      "455 0.0406206\n",
      "456 0.0406191\n",
      "457 0.0406175\n",
      "458 0.040616\n",
      "459 0.0406144\n",
      "460 0.0406129\n",
      "0.0406115\n",
      "461 0.0406115\n",
      "462 0.0406101\n",
      "463 0.0406087\n",
      "464 0.0406073\n",
      "465 0.0406056\n",
      "466 0.0406039\n",
      "467 0.0406024\n",
      "468 0.0406011\n",
      "469 0.0405998\n",
      "470 0.0405982\n",
      "0.0405969\n",
      "471 0.0405969\n",
      "472 0.0405954\n",
      "473 0.040594\n",
      "474 0.0405928\n",
      "475 0.0405914\n",
      "476 0.0405899\n",
      "477 0.0405885\n",
      "478 0.0405871\n",
      "479 0.0405856\n",
      "480 0.0405844\n",
      "0.0405829\n",
      "481 0.0405829\n",
      "482 0.0405817\n",
      "483 0.0405802\n",
      "484 0.0405788\n",
      "485 0.0405776\n",
      "486 0.0405764\n",
      "487 0.040575\n",
      "488 0.0405738\n",
      "489 0.0405724\n",
      "490 0.0405709\n",
      "0.0405695\n",
      "491 0.0405695\n",
      "492 0.0405683\n",
      "493 0.0405669\n",
      "494 0.0405656\n",
      "495 0.0405644\n",
      "496 0.0405632\n",
      "497 0.040562\n",
      "498 0.0405607\n",
      "499 0.0405594\n",
      "500 0.040558\n",
      "0.0405569\n",
      "501 0.0405569\n",
      "502 0.0405556\n",
      "503 0.0405542\n",
      "504 0.040553\n",
      "505 0.0405516\n",
      "506 0.0405505\n",
      "507 0.0405494\n",
      "508 0.0405482\n",
      "509 0.0405469\n",
      "510 0.0405457\n",
      "0.0405446\n",
      "511 0.0405446\n",
      "512 0.0405433\n",
      "513 0.0405421\n",
      "514 0.0405409\n",
      "515 0.0405399\n",
      "516 0.0405387\n",
      "517 0.0405376\n",
      "518 0.0405363\n",
      "519 0.0405352\n",
      "520 0.0405341\n",
      "0.0405328\n",
      "521 0.0405328\n",
      "522 0.0405316\n",
      "523 0.0405306\n",
      "524 0.0405294\n",
      "525 0.0405283\n",
      "526 0.0405273\n",
      "527 0.0405262\n",
      "528 0.0405252\n",
      "529 0.0405241\n",
      "530 0.0405231\n",
      "0.0405218\n",
      "531 0.0405218\n",
      "532 0.0405207\n",
      "533 0.0405195\n",
      "534 0.0405184\n",
      "535 0.0405171\n",
      "536 0.040516\n",
      "537 0.0405147\n",
      "538 0.0405137\n",
      "539 0.0405128\n",
      "540 0.0405119\n",
      "0.0405109\n",
      "541 0.0405109\n",
      "542 0.0405099\n",
      "543 0.0405092\n",
      "544 0.0405083\n",
      "545 0.0405072\n",
      "546 0.040506\n",
      "547 0.0405048\n",
      "548 0.0405038\n",
      "549 0.0405028\n",
      "550 0.040502\n",
      "0.040501\n",
      "551 0.040501\n",
      "552 0.0404999\n",
      "553 0.0404989\n",
      "554 0.0404979\n",
      "555 0.0404967\n",
      "556 0.0404957\n",
      "557 0.0404949\n",
      "558 0.0404941\n",
      "559 0.0404932\n",
      "560 0.0404923\n",
      "0.0404912\n",
      "561 0.0404912\n",
      "562 0.0404903\n",
      "563 0.0404892\n",
      "564 0.0404883\n",
      "565 0.0404872\n",
      "566 0.0404864\n",
      "567 0.0404855\n",
      "568 0.0404847\n",
      "569 0.0404837\n",
      "570 0.0404828\n",
      "0.0404819\n",
      "571 0.0404819\n",
      "572 0.040481\n",
      "573 0.0404802\n",
      "574 0.0404793\n",
      "575 0.0404783\n",
      "576 0.0404771\n",
      "577 0.0404761\n",
      "578 0.0404752\n",
      "579 0.0404743\n",
      "580 0.0404735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0404728\n",
      "581 0.0404728\n",
      "582 0.0404719\n",
      "583 0.0404712\n",
      "584 0.0404704\n",
      "585 0.0404697\n",
      "586 0.040469\n",
      "587 0.040468\n",
      "588 0.040467\n",
      "589 0.0404662\n",
      "590 0.0404652\n",
      "0.0404645\n",
      "591 0.0404645\n",
      "592 0.0404636\n",
      "593 0.040463\n",
      "594 0.0404621\n",
      "595 0.0404612\n",
      "596 0.0404603\n",
      "597 0.0404593\n",
      "598 0.0404585\n",
      "599 0.0404579\n",
      "600 0.0404571\n",
      "0.0404564\n",
      "601 0.0404564\n",
      "602 0.0404557\n",
      "603 0.0404549\n",
      "604 0.0404541\n",
      "605 0.0404534\n",
      "606 0.0404526\n",
      "607 0.040452\n",
      "608 0.0404511\n",
      "609 0.0404504\n",
      "610 0.0404496\n",
      "0.0404487\n",
      "611 0.0404487\n",
      "612 0.0404478\n",
      "613 0.0404472\n",
      "614 0.0404464\n",
      "615 0.0404457\n",
      "616 0.040445\n",
      "617 0.0404441\n",
      "618 0.0404434\n",
      "619 0.0404428\n",
      "620 0.040442\n",
      "0.0404412\n",
      "621 0.0404412\n",
      "622 0.0404405\n",
      "623 0.0404399\n",
      "624 0.0404393\n",
      "625 0.0404387\n",
      "626 0.040438\n",
      "627 0.0404372\n",
      "628 0.0404365\n",
      "629 0.0404358\n",
      "630 0.0404351\n",
      "0.0404344\n",
      "631 0.0404344\n",
      "632 0.0404338\n",
      "633 0.0404332\n",
      "634 0.0404325\n",
      "635 0.0404318\n",
      "636 0.0404311\n",
      "637 0.0404304\n",
      "638 0.0404295\n",
      "639 0.040429\n",
      "640 0.0404284\n",
      "0.0404276\n",
      "641 0.0404276\n",
      "642 0.040427\n",
      "643 0.0404265\n",
      "644 0.0404257\n",
      "645 0.0404253\n",
      "646 0.0404247\n",
      "647 0.0404242\n",
      "648 0.0404236\n",
      "649 0.040423\n",
      "650 0.0404223\n",
      "0.0404216\n",
      "651 0.0404216\n",
      "652 0.0404211\n",
      "653 0.0404203\n",
      "654 0.0404197\n",
      "655 0.0404192\n",
      "656 0.0404187\n",
      "657 0.040418\n",
      "658 0.0404175\n",
      "659 0.0404169\n",
      "660 0.0404163\n",
      "0.0404156\n",
      "661 0.0404156\n",
      "662 0.0404149\n",
      "663 0.0404143\n",
      "664 0.0404136\n",
      "665 0.0404132\n",
      "666 0.0404126\n",
      "667 0.040412\n",
      "668 0.0404117\n",
      "669 0.0404109\n",
      "670 0.0404103\n",
      "0.0404098\n",
      "671 0.0404098\n",
      "672 0.0404093\n",
      "673 0.0404088\n",
      "674 0.0404082\n",
      "675 0.0404075\n",
      "676 0.0404071\n",
      "677 0.0404063\n",
      "678 0.0404058\n",
      "679 0.0404053\n",
      "680 0.0404047\n",
      "0.0404041\n",
      "681 0.0404041\n",
      "682 0.0404036\n",
      "683 0.0404031\n",
      "684 0.0404026\n",
      "685 0.0404021\n",
      "686 0.0404017\n",
      "687 0.0404013\n",
      "688 0.0404009\n",
      "689 0.0404005\n",
      "690 0.0403998\n",
      "0.0403991\n",
      "691 0.0403991\n",
      "692 0.0403986\n",
      "693 0.0403982\n",
      "694 0.0403977\n",
      "695 0.0403972\n",
      "696 0.0403966\n",
      "697 0.0403961\n",
      "698 0.0403956\n",
      "699 0.0403951\n",
      "700 0.0403947\n",
      "0.0403943\n",
      "701 0.0403943\n",
      "702 0.0403939\n",
      "703 0.0403934\n",
      "704 0.0403928\n",
      "705 0.0403923\n",
      "706 0.0403919\n",
      "707 0.0403914\n",
      "708 0.0403908\n",
      "709 0.0403902\n",
      "710 0.0403899\n",
      "0.0403893\n",
      "711 0.0403893\n",
      "712 0.0403888\n",
      "713 0.0403885\n",
      "714 0.0403881\n",
      "715 0.0403877\n",
      "716 0.0403872\n",
      "717 0.0403867\n",
      "718 0.0403863\n",
      "719 0.0403858\n",
      "720 0.0403854\n",
      "0.040385\n",
      "721 0.040385\n",
      "722 0.0403845\n",
      "723 0.0403841\n",
      "724 0.0403837\n",
      "725 0.0403833\n",
      "726 0.040383\n",
      "727 0.0403825\n",
      "728 0.0403822\n",
      "729 0.0403816\n",
      "730 0.0403812\n",
      "0.0403807\n",
      "731 0.0403807\n",
      "732 0.0403804\n",
      "733 0.04038\n",
      "734 0.0403796\n",
      "735 0.0403792\n",
      "736 0.0403788\n",
      "737 0.0403784\n",
      "738 0.0403778\n",
      "739 0.0403775\n",
      "740 0.0403771\n",
      "0.0403768\n",
      "741 0.0403768\n",
      "742 0.0403764\n",
      "743 0.0403761\n",
      "744 0.0403757\n",
      "745 0.0403752\n",
      "746 0.0403747\n",
      "747 0.0403743\n",
      "748 0.0403741\n",
      "749 0.0403737\n",
      "750 0.0403733\n",
      "0.040373\n",
      "751 0.040373\n",
      "752 0.0403727\n",
      "753 0.0403723\n",
      "754 0.040372\n",
      "755 0.0403716\n",
      "756 0.0403712\n",
      "757 0.0403708\n",
      "758 0.0403706\n",
      "759 0.0403702\n",
      "760 0.0403699\n",
      "0.0403696\n",
      "761 0.0403696\n",
      "762 0.0403692\n",
      "763 0.0403687\n",
      "764 0.0403683\n",
      "765 0.040368\n",
      "766 0.0403677\n",
      "767 0.0403674\n",
      "768 0.040367\n",
      "769 0.0403666\n",
      "770 0.0403663\n",
      "0.040366\n",
      "771 0.040366\n",
      "772 0.0403658\n",
      "773 0.0403653\n",
      "774 0.040365\n",
      "775 0.0403647\n",
      "776 0.0403644\n",
      "777 0.0403641\n",
      "778 0.0403636\n",
      "779 0.0403633\n",
      "780 0.0403631\n",
      "0.0403627\n",
      "781 0.0403627\n",
      "782 0.0403626\n",
      "783 0.0403622\n",
      "784 0.0403618\n",
      "785 0.0403613\n",
      "786 0.040361\n",
      "787 0.0403608\n",
      "788 0.0403605\n",
      "789 0.0403602\n",
      "790 0.04036\n",
      "0.0403598\n",
      "791 0.0403598\n",
      "792 0.0403595\n",
      "793 0.0403593\n",
      "794 0.0403589\n",
      "795 0.0403585\n",
      "796 0.0403582\n",
      "797 0.0403577\n",
      "798 0.0403574\n",
      "799 0.0403571\n",
      "800 0.0403568\n",
      "0.0403565\n",
      "801 0.0403565\n",
      "802 0.0403562\n",
      "803 0.0403559\n",
      "804 0.0403558\n",
      "805 0.0403555\n",
      "806 0.0403551\n",
      "807 0.040355\n",
      "808 0.0403547\n",
      "809 0.0403544\n",
      "810 0.0403541\n",
      "0.0403539\n",
      "811 0.0403539\n",
      "812 0.0403537\n",
      "813 0.0403535\n",
      "814 0.0403532\n",
      "815 0.040353\n",
      "816 0.0403528\n",
      "817 0.0403526\n",
      "818 0.0403524\n",
      "819 0.0403521\n",
      "820 0.0403518\n",
      "0.0403515\n",
      "821 0.0403515\n",
      "822 0.0403512\n",
      "823 0.0403509\n",
      "824 0.0403507\n",
      "825 0.0403505\n",
      "826 0.0403501\n",
      "827 0.0403498\n",
      "828 0.0403496\n",
      "829 0.0403493\n",
      "830 0.0403491\n",
      "0.040349\n",
      "831 0.040349\n",
      "832 0.0403488\n",
      "833 0.0403485\n",
      "834 0.0403481\n",
      "835 0.0403478\n",
      "836 0.0403475\n",
      "837 0.0403472\n",
      "838 0.0403469\n",
      "839 0.0403467\n",
      "840 0.0403465\n",
      "0.0403462\n",
      "841 0.0403462\n",
      "842 0.0403458\n",
      "843 0.0403458\n",
      "844 0.0403458\n",
      "845 0.0403456\n",
      "846 0.0403454\n",
      "847 0.0403452\n",
      "848 0.0403449\n",
      "849 0.0403447\n",
      "850 0.0403446\n",
      "0.0403445\n",
      "851 0.0403445\n",
      "852 0.0403441\n",
      "853 0.0403439\n",
      "854 0.0403436\n",
      "855 0.0403434\n",
      "856 0.0403432\n",
      "857 0.0403431\n",
      "858 0.0403427\n",
      "859 0.0403425\n",
      "860 0.0403422\n",
      "0.0403421\n",
      "861 0.0403421\n",
      "862 0.0403418\n",
      "863 0.0403414\n",
      "864 0.0403411\n",
      "865 0.0403407\n",
      "866 0.0403406\n",
      "867 0.0403404\n",
      "868 0.0403403\n",
      "869 0.0403403\n",
      "870 0.0403401\n",
      "0.04034\n",
      "871 0.04034\n",
      "872 0.0403398\n",
      "873 0.0403396\n",
      "874 0.0403394\n",
      "875 0.0403393\n",
      "876 0.0403391\n",
      "877 0.0403389\n",
      "878 0.0403387\n",
      "879 0.0403386\n",
      "880 0.0403385\n",
      "0.0403383\n",
      "881 0.0403383\n",
      "882 0.0403382\n",
      "883 0.040338\n",
      "884 0.0403379\n",
      "885 0.0403376\n",
      "886 0.0403374\n",
      "887 0.0403372\n",
      "888 0.0403371\n",
      "889 0.0403368\n",
      "890 0.0403364\n",
      "0.0403361\n",
      "891 0.0403361\n",
      "892 0.0403359\n",
      "893 0.0403357\n",
      "894 0.0403356\n",
      "895 0.0403355\n",
      "896 0.0403354\n",
      "897 0.0403352\n",
      "898 0.0403351\n",
      "899 0.040335\n",
      "900 0.0403347\n",
      "0.0403345\n",
      "901 0.0403345\n",
      "902 0.0403343\n",
      "903 0.0403342\n",
      "904 0.0403342\n",
      "905 0.0403342\n",
      "906 0.040334\n",
      "907 0.0403338\n",
      "908 0.0403337\n",
      "909 0.0403336\n",
      "910 0.0403335\n",
      "0.0403332\n",
      "911 0.0403332\n",
      "912 0.040333\n",
      "913 0.0403328\n",
      "914 0.0403325\n",
      "915 0.0403324\n",
      "916 0.0403322\n",
      "917 0.0403322\n",
      "918 0.040332\n",
      "919 0.0403317\n",
      "920 0.0403315\n",
      "0.0403314\n",
      "921 0.0403314\n",
      "922 0.0403312\n",
      "923 0.0403311\n",
      "924 0.040331\n",
      "925 0.0403308\n",
      "926 0.0403306\n",
      "927 0.0403304\n",
      "928 0.0403303\n",
      "929 0.0403302\n",
      "930 0.0403302\n",
      "0.0403301\n",
      "931 0.0403301\n",
      "932 0.04033\n",
      "933 0.0403299\n",
      "934 0.0403297\n",
      "935 0.0403296\n",
      "936 0.0403294\n",
      "937 0.0403292\n",
      "938 0.0403289\n",
      "939 0.0403289\n",
      "940 0.0403289\n",
      "0.0403288\n",
      "941 0.0403288\n",
      "942 0.0403286\n",
      "943 0.0403285\n",
      "944 0.0403282\n",
      "945 0.0403281\n",
      "946 0.0403279\n",
      "947 0.0403278\n",
      "948 0.0403277\n",
      "949 0.0403277\n",
      "950 0.0403274\n",
      "0.0403274\n",
      "951 0.0403274\n",
      "952 0.0403274\n",
      "953 0.0403271\n",
      "954 0.0403269\n",
      "955 0.0403267\n",
      "956 0.0403264\n",
      "957 0.0403264\n",
      "958 0.0403264\n",
      "959 0.0403263\n",
      "960 0.0403262\n",
      "0.0403263\n",
      "961 0.0403263\n",
      "962 0.0403263\n",
      "963 0.0403263\n",
      "964 0.0403261\n",
      "965 0.0403261\n",
      "966 0.0403258\n",
      "967 0.0403256\n",
      "968 0.0403255\n",
      "969 0.0403254\n",
      "970 0.0403253\n",
      "0.040325\n",
      "971 0.040325\n",
      "972 0.0403249\n",
      "973 0.0403248\n",
      "974 0.0403247\n",
      "975 0.0403246\n",
      "976 0.0403247\n",
      "977 0.0403245\n",
      "978 0.0403243\n",
      "979 0.0403242\n",
      "980 0.040324\n",
      "0.0403238\n",
      "981 0.0403238\n",
      "982 0.0403237\n",
      "983 0.0403237\n",
      "984 0.0403238\n",
      "985 0.0403237\n",
      "986 0.0403236\n",
      "987 0.0403235\n",
      "988 0.0403234\n",
      "989 0.0403231\n",
      "990 0.040323\n",
      "0.0403229\n",
      "991 0.0403229\n",
      "992 0.0403227\n",
      "993 0.0403226\n",
      "994 0.0403225\n",
      "995 0.0403224\n",
      "996 0.0403224\n",
      "997 0.0403221\n",
      "998 0.040322\n",
      "999 0.0403221\n",
      "1000 0.040322\n",
      "0.040322\n"
     ]
    }
   ],
   "source": [
    "for i in range(1001):\n",
    "    _, l = sess.run([opt, loss], feed_dict={input_X:trainX, input_y:train_y})\n",
    "    print(i, l)\n",
    "    if i % 10 == 0:\n",
    "        m = sess.run(mse, feed_dict={input_X:trainX, input_y:train_y})\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get prediction and write the answer\n",
    "test1_pred = sess.run(pred_y, feed_dict={input_X:test1X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.84237456],\n",
       "       [ 2.84236979],\n",
       "       [ 2.84035826],\n",
       "       [ 2.8403511 ],\n",
       "       [ 2.84015203],\n",
       "       [ 2.84074783],\n",
       "       [ 2.8388567 ],\n",
       "       [ 2.83939934],\n",
       "       [ 2.83939934],\n",
       "       [ 2.83866572],\n",
       "       [ 2.83866572],\n",
       "       [ 2.83990717],\n",
       "       [ 2.83958435],\n",
       "       [ 2.83885407],\n",
       "       [ 2.83991003],\n",
       "       [ 2.83977866],\n",
       "       [ 2.83977866],\n",
       "       [ 2.84034777],\n",
       "       [ 2.8397789 ],\n",
       "       [ 2.83976889],\n",
       "       [ 2.83897781],\n",
       "       [ 2.83976889],\n",
       "       [ 2.83976889],\n",
       "       [ 2.83992815],\n",
       "       [ 2.83976936],\n",
       "       [ 2.83860087],\n",
       "       [ 2.8397696 ],\n",
       "       [ 2.8397696 ],\n",
       "       [ 2.83992839],\n",
       "       [ 2.83929944],\n",
       "       [ 2.83930421],\n",
       "       [ 2.83930397],\n",
       "       [ 2.84009862],\n",
       "       [ 2.84009981],\n",
       "       [ 2.83917332],\n",
       "       [ 2.83917499],\n",
       "       [ 2.83868575],\n",
       "       [ 2.8388114 ],\n",
       "       [ 2.83868575],\n",
       "       [ 2.83868575],\n",
       "       [ 2.84010696],\n",
       "       [ 2.83868146],\n",
       "       [ 2.83881187],\n",
       "       [ 2.83849788],\n",
       "       [ 2.83899808],\n",
       "       [ 2.83849835],\n",
       "       [ 2.83898783],\n",
       "       [ 2.83849931],\n",
       "       [ 2.838974  ],\n",
       "       [ 2.83849978],\n",
       "       [ 2.83896828],\n",
       "       [ 2.83986044],\n",
       "       [ 2.84061766],\n",
       "       [ 2.83896947],\n",
       "       [ 2.83896947],\n",
       "       [ 2.83896971],\n",
       "       [ 2.83936596],\n",
       "       [ 2.84136844],\n",
       "       [ 2.8400991 ],\n",
       "       [ 2.84136868],\n",
       "       [ 2.8391037 ],\n",
       "       [ 2.84069991],\n",
       "       [ 2.84070039],\n",
       "       [ 2.84098887],\n",
       "       [ 2.83970428],\n",
       "       [ 2.84120703],\n",
       "       [ 2.84120274],\n",
       "       [ 2.84055591],\n",
       "       [ 2.83920622],\n",
       "       [ 2.84055471],\n",
       "       [ 2.84066629],\n",
       "       [ 2.84304452],\n",
       "       [ 2.84304476],\n",
       "       [ 2.84168148],\n",
       "       [ 2.84094739],\n",
       "       [ 2.83965325],\n",
       "       [ 2.83885527],\n",
       "       [ 2.84015059],\n",
       "       [ 2.84304452],\n",
       "       [ 2.84230542],\n",
       "       [ 2.84359384],\n",
       "       [ 2.84359384],\n",
       "       [ 2.84230566],\n",
       "       [ 2.8410995 ],\n",
       "       [ 2.84110761],\n",
       "       [ 2.8403573 ],\n",
       "       [ 2.84035969],\n",
       "       [ 2.84027195],\n",
       "       [ 2.83966589],\n",
       "       [ 2.83893657],\n",
       "       [ 2.83890486],\n",
       "       [ 2.84029078],\n",
       "       [ 2.83975649],\n",
       "       [ 2.83890295],\n",
       "       [ 2.83963299],\n",
       "       [ 2.84028721],\n",
       "       [ 2.84035802],\n",
       "       [ 2.83890319],\n",
       "       [ 2.84375691],\n",
       "       [ 2.83963323]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "test1_pred = test1_pred.reshape(100)\n",
    "print(test_id_1.shape, test1_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save = pd.DataFrame({'ID':test_id_1, 'value':test1_pred})\n",
    "save.to_csv('answer.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.200769269595 2.8461873043805452\n"
     ]
    }
   ],
   "source": [
    "print(y_s, y_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.454555830692319 2.3268455638817764\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_y), np.min(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regression by machinr learn\n",
    "def MSE_np(y, y_pred):\n",
    "    return np.mean(np.square(y-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(trainX, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_pred = lr.predict(test1X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_pred = test1_pred.reshape(100)\n",
    "save = pd.DataFrame({'ID':test_id_1, 'value':test1_pred})\n",
    "save.to_csv('answer.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = lr.predict(trainX)\n",
    "mse_tr = MSE_np(train_y, train_pred)\n",
    "print(mse_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magnusterra/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX, vaX, tr_y, va_y = train_test_split(trainX, train_y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036568362863228905\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(trX, tr_y)\n",
    "tr_p = lr.predict(trX)\n",
    "print(MSE_np(tr_y, tr_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029323336623688974\n"
     ]
    }
   ],
   "source": [
    "va_p = lr.predict(vaX)\n",
    "print(MSE_np(va_y, va_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
